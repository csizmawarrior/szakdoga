{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import *\n",
    "import os\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 13209, 16555, 10174,   102]])\n",
      "['Au', '##tó', '##k']\n"
     ]
    }
   ],
   "source": [
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-multilingual-cased'),\n",
    "         ]\n",
    "# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`\n",
    "# Let's encode some text in a sequence of hidden-states using each model:\n",
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    # Load pretrained model/tokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    # Encode text\n",
    "    input_ids = torch.tensor([tokenizer.encode(\"Autók\", add_special_tokens=True)]) \n",
    "    # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
    "    print(input_ids)\n",
    "    print(tokenizer.tokenize(\"Autók\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=[]\n",
    "with open(os.path.join(\"C:/Users/pc/OneDrive - Budapesti Műszaki és Gazdaságtudományi Egyetem/szakdoga\", \"text.txt\"), encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        lines.append(line)\n",
    "linesnp=numpy.array(lines)\n",
    "linesnp.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=[]\n",
    "with open(os.path.join(\"C:/Users/pc/OneDrive - Budapesti Műszaki és Gazdaságtudományi Egyetem/szakdoga\", \"dev.tsv\"), encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        oneLine=line.split(\"\\t\", 4)\n",
    "        lines.append(oneLine)\n",
    "linesnp=numpy.array(lines)\n",
    "linesnp.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utasításra'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesnp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ec1b1179409b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "len[lines[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=['Kizárólag', 'felső', 'utasításra', 'cselekedtünk', '-', 'mondják', 'egybehangzóan', 'az', 'osztrák', 'külügyi', ',', 'belügyi', 'és', 'igazságügyi', 'tárca', 'illetékesei', ',', 'miután', 'a', 'múlt', 'hét', 'szerdán', 'ők', 'hajtsák', 'végre', 'az', '57', 'esztendős', 'Talics', 'letartóztatását', '.\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 28941, 79457, 25373, 14520, 102], [101, 86546, 102], [101, 11735, 10403, 52215, 10288, 102], [101, 171, 68640, 30511, 74873, 17761, 102], [101, 103, 102], [101, 52607, 20068, 102], [101, 11215, 11044, 30222, 16993, 10206, 102], [101, 10360, 102], [101, 65913, 102], [101, 179, 16599, 55964, 102], [101, 117, 102], [101, 65817, 55964, 102], [101, 10256, 102], [101, 72811, 47799, 22180, 55964, 102], [101, 30185, 22549, 102], [101, 32941, 61906, 21885, 11998, 102], [101, 117, 102], [101, 61351, 102], [101, 169, 102], [101, 181, 49227, 10123, 102], [101, 62221, 102], [101, 61048, 13819, 11169, 102], [101, 341, 10174, 102], [101, 10228, 26694, 10107, 12951, 102], [101, 190, 106546, 102], [101, 10360, 102], [101, 11817, 102], [101, 10196, 44829, 31077, 10107, 102], [101, 24471, 16981, 102], [101, 12946, 10976, 83355, 13485, 28199, 102], [101, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "tokenLines=[]\n",
    "for charword in test :\n",
    "    tokenLines.append(tokenizer.tokenize(charword))\n",
    "\n",
    "\n",
    "inputs=tokenizer(test)    \n",
    "#inputs = tokenizer.tokenize(lines[0])\n",
    "#outputs = model(numpy.array([tokenLines,linesnp[0][1], linesnp[0][2], linesnp[0][3]]))\n",
    "print(inputs)\n",
    "#print(\"OUTPUTS\")\n",
    "#print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K']\n",
      "['i']\n",
      "['z']\n",
      "['á']\n",
      "['r']\n",
      "['ó']\n",
      "['l']\n",
      "['a']\n",
      "['g']\n",
      "[]\n",
      "['f']\n",
      "['e']\n",
      "['l']\n",
      "['s']\n",
      "['ő']\n",
      "[]\n",
      "['u']\n",
      "['t']\n",
      "['a']\n",
      "['s']\n",
      "['í']\n",
      "['t']\n",
      "['á']\n",
      "['s']\n",
      "['r']\n",
      "['a']\n",
      "[]\n",
      "['c']\n",
      "['s']\n",
      "['e']\n",
      "['l']\n",
      "['e']\n",
      "['k']\n",
      "['e']\n",
      "['d']\n",
      "['t']\n",
      "['ü']\n",
      "['n']\n",
      "['k']\n",
      "[]\n",
      "['[UNK]']\n",
      "[]\n",
      "['m']\n",
      "['o']\n",
      "['n']\n",
      "['d']\n",
      "['j']\n",
      "['á']\n",
      "['k']\n",
      "[]\n",
      "['e']\n",
      "['g']\n",
      "['y']\n",
      "['b']\n",
      "['e']\n",
      "['h']\n",
      "['a']\n",
      "['n']\n",
      "['g']\n",
      "['z']\n",
      "['ó']\n",
      "['a']\n",
      "['n']\n",
      "[]\n",
      "['a']\n",
      "['z']\n",
      "[]\n",
      "['o']\n",
      "['s']\n",
      "['z']\n",
      "['t']\n",
      "['r']\n",
      "['á']\n",
      "['k']\n",
      "[]\n",
      "['k']\n",
      "['ü']\n",
      "['l']\n",
      "['ü']\n",
      "['g']\n",
      "['y']\n",
      "['i']\n",
      "[]\n",
      "[',']\n",
      "[]\n",
      "['b']\n",
      "['e']\n",
      "['l']\n",
      "['ü']\n",
      "['g']\n",
      "['y']\n",
      "['i']\n",
      "[]\n",
      "['é']\n",
      "['s']\n",
      "[]\n",
      "['i']\n",
      "['g']\n",
      "['a']\n",
      "['z']\n",
      "['s']\n",
      "['á']\n",
      "['g']\n",
      "['ü']\n",
      "['g']\n",
      "['y']\n",
      "['i']\n",
      "[]\n",
      "['t']\n",
      "['á']\n",
      "['r']\n",
      "['c']\n",
      "['a']\n",
      "[]\n",
      "['i']\n",
      "['l']\n",
      "['l']\n",
      "['e']\n",
      "['t']\n",
      "['é']\n",
      "['k']\n",
      "['e']\n",
      "['s']\n",
      "['e']\n",
      "['i']\n",
      "[]\n",
      "[',']\n",
      "[]\n",
      "['m']\n",
      "['i']\n",
      "['u']\n",
      "['t']\n",
      "['á']\n",
      "['n']\n",
      "[]\n",
      "['a']\n",
      "[]\n",
      "['m']\n",
      "['ú']\n",
      "['l']\n",
      "['t']\n",
      "[]\n",
      "['h']\n",
      "['é']\n",
      "['t']\n",
      "[]\n",
      "['s']\n",
      "['z']\n",
      "['e']\n",
      "['r']\n",
      "['d']\n",
      "['á']\n",
      "['n']\n",
      "[]\n",
      "['ő']\n",
      "['k']\n",
      "[]\n",
      "['h']\n",
      "['a']\n",
      "['j']\n",
      "['t']\n",
      "['o']\n",
      "['t']\n",
      "['t']\n",
      "['á']\n",
      "['k']\n",
      "[]\n",
      "['v']\n",
      "['é']\n",
      "['g']\n",
      "['r']\n",
      "['e']\n",
      "[]\n",
      "['a']\n",
      "['z']\n",
      "[]\n",
      "['5']\n",
      "['7']\n",
      "[]\n",
      "['e']\n",
      "['s']\n",
      "['z']\n",
      "['t']\n",
      "['e']\n",
      "['n']\n",
      "['d']\n",
      "['ő']\n",
      "['s']\n",
      "[]\n",
      "['T']\n",
      "['a']\n",
      "['l']\n",
      "['i']\n",
      "['c']\n",
      "['s']\n",
      "[]\n",
      "['l']\n",
      "['e']\n",
      "['t']\n",
      "['a']\n",
      "['r']\n",
      "['t']\n",
      "['ó']\n",
      "['z']\n",
      "['t']\n",
      "['a']\n",
      "['t']\n",
      "['á']\n",
      "['s']\n",
      "['á']\n",
      "['t']\n",
      "[]\n",
      "['.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for tokenized in tokenLines:\n",
    "    print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mind', 'a', 'szakszervezeti', 'képviselők', ',', 'mind', 'az', 'Adatvédelmi', 'Hivatal', 'tehetetlen', 'a', 'munkahelyek', '##en', 'egyre', 'terjedő', 'megfigyelése', '##k', '##kel', 'szemben', ',', 'miután', 'nincsenek', 'törvényi', 'korlát', '##ai', 'a', 'kamerák', 'működtet', '##ésének', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "inputs = tokenizer.tokenize(lines[146])\n",
    "#outputs = model(**inputs)\n",
    "print(inputs)\n",
    "#print(\"OUTPUTS\")\n",
    "#print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
